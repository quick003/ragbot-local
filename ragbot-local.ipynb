# -----------------------------------------------------------
# ğŸ““ CELLÂ 1 â€“Â System setup: APT packages, Ollama, CUDA drivers
# -----------------------------------------------------------
!sudo apt-get update -y
!sudo apt-get install -y curl pciutils lsb-release
!curl -fsSL https://ollama.com/install.sh | sh                # installs ollama CLI

# (Optional) show GPU status â€“Â handy for debugging
!nvidia-smi || true

# -----------------------------------------------------------
# ğŸ““ CELLÂ 2 â€“Â Python dependencies
# -----------------------------------------------------------
!pip install -q \
    langchain-core \
    langchain-community \
    langchain-chroma \
    langchain-ollama \
    langchain-huggingface \
    chromadb \
    gradio \
    pymupdf \
    unstructured[docx] \
    tqdm

# -----------------------------------------------------------
# ğŸ““ CELLÂ 3 â€“Â Pull the LLM & start Ollama server
# -----------------------------------------------------------
import subprocess, threading, requests, time, os

MODEL_NAME = "mistral:7b"    # ğŸ”„ change if you want (e.g. "phi", "llama3", etc.)

def _serve():
    subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

threading.Thread(target=_serve, daemon=True).start()

# Wait until Ollama REST endpoint is up
for _ in range(20):
    try:
        if requests.get("http://localhost:11434").ok:
            break
    except:
        time.sleep(1)
else:
    raise RuntimeError("Ollama failed to start.")

# Pull model once
!ollama pull {MODEL_NAME}
print("âœ… Ollama ready with", MODEL_NAME)

# -----------------------------------------------------------
# ğŸ““ CELL 4 â€“ Paths & global objects
# -----------------------------------------------------------
import warnings
from pathlib import Path
from typing import List

# LangChain imports
from langchain_community.document_loaders import (
    PyPDFLoader,
    CSVLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
    UnstructuredWordDocumentLoader  # âœ… Added DOCX loader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama.llms import OllamaLLM
from langchain.chains import RetrievalQA

# Paths
DB_DIR = Path("/content/db")
DATA_DIR = Path("/content/datas")
DB_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)

# Globals
EMBEDDER = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
SPLITTER = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)

warnings.filterwarnings("ignore")


# -----------------------------------------------------------
# ğŸ““ CELL 5 â€“ Utility functions (read docs, build index, etc.)
# -----------------------------------------------------------
def read_files_local(filepaths: List[Path]):
    """Parse multiple local documents into LangChain Document objects."""
    docs = []
    for path in filepaths:
        ext = path.suffix.lower()
        try:
            if ext == ".pdf":
                loader = PyPDFLoader(str(path))
            elif ext == ".csv":
                loader = CSVLoader(str(path))
            elif ext == ".md":
                loader = UnstructuredMarkdownLoader(str(path))
            elif ext == ".txt":
                loader = TextLoader(str(path), encoding="utf-8")
            elif ext == ".docx":  # âœ… NEW
                loader = UnstructuredWordDocumentLoader(str(path))
            else:
                print(f"[!] Skipping unsupported file: {path.name}")
                continue
            for doc in loader.load():
                doc.metadata["source"] = path.name
                docs.append(doc)
        except Exception as e:
            print(f"[!] Failed to parse {path.name}: {e}")
    return docs

def build_index_once():
    """Build Chroma index on first run (skips if already exists)."""
    if any(DB_DIR.iterdir()):
        print("ğŸ”¹ Vectorstore already exists â€“ skipping ingest.")
        return
    files = [p for p in DATA_DIR.iterdir() if not p.name.startswith(".")]
    if not files:
        raise RuntimeError(f"No documents found in {DATA_DIR}. Upload before running.")
    print(f"ğŸ›   Ingesting {len(files)} file(s)â€¦")
    docs   = read_files_local(files)
    chunks = SPLITTER.split_documents(docs)
    Chroma.from_documents(
        chunks,
        embedding=EMBEDDER,
        persist_directory=str(DB_DIR),
        collection_name="rag-docs"
    )
    print(f"âœ… Vectorstore ready with {len(chunks)} chunks.")

# (Re)build the index
build_index_once()

# -----------------------------------------------------------
# ğŸ““ CELLÂ 6 â€“Â Create global QA chain (with safe prompt and source viewing)
# -----------------------------------------------------------
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# ğŸ”¹ Define safe prompt that restricts model to use only retrieved context
CUSTOM_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "You are a helpful assistant. Use ONLY the following context to answer the question. "
        "If the answer is not in the context, respond with: 'I don't know based on the provided documents.'\n\n"
        "Context:\n{context}\n\n"
        "Question: {question}\nAnswer:"
    )
)

# ğŸ”¹ Create vector retriever
VECTORSTORE = Chroma(
    persist_directory=str(DB_DIR),
    embedding_function=EMBEDDER,
    collection_name="rag-docs"
)
RETRIEVER = VECTORSTORE.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 25}
)

# ğŸ”¹ Create QA chain with safe prompt and source visibility
LLM = OllamaLLM(model=MODEL_NAME)
QA_CHAIN = RetrievalQA.from_chain_type(
    llm=LLM,
    retriever=RETRIEVER,
    chain_type="stuff",
    return_source_documents=True,  # âœ… Enables inspection of source chunks
    chain_type_kwargs={"prompt": CUSTOM_PROMPT}
)

print("âœ… QA chain initialized with context-only guardrails.")

# -----------------------------------------------------------
# ğŸ““ CELLÂ 7 â€“Â Gradio app with perâ€‘user session isolation
#   (now shows retrieved context for every answer)
# -----------------------------------------------------------
import gradio as gr

def chat_fn(state, user_msg):
    """
    state:  list[tuple[str, str]]  â€“ running chat history
    user_msg: str                  â€“ new question from the user
    """
    user_msg = (user_msg or "").strip()
    if not user_msg:
        return state, state  # nothing to do

    # â”€â”€ 1ï¸âƒ£  Add a placeholder so the UI doesn't look frozen
    state.append((user_msg, "â³ â€¦thinkingâ€¦"))
    yield state, state

    # â”€â”€ 2ï¸âƒ£  Run the RAG chain
    try:
        result   = QA_CHAIN.invoke({"query": user_msg})
        answer   = result["result"]
        sources  = result.get("source_documents", [])

        # Build a readable context display (first 2â€‘3 lines of each chunk)
        if sources:
            context_snippets = []
            for doc in sources:
                snippet = (
                    doc.page_content.strip().split("\n", 3)[:3]  # first lines only
                )
                context_snippets.append(" â€¢ ".join(snippet))
            answer += "\n\nğŸ“„ **Retrieved context:**\n" + "\n".join(
                f"- {s}" for s in context_snippets
            )

    except Exception as e:
        answer = f"âš ï¸ {type(e).__name__}: {e}"

    # â”€â”€ 3ï¸âƒ£  Replace placeholder with final answer
    state[-1] = (user_msg, answer)
    yield state, state


with gr.Blocks(title="RAG Chatbot (Preâ€‘loaded docs)") as demo:
    gr.Markdown("## ğŸ“š Ask anything about the preâ€‘loaded documents")
    chatbot   = gr.Chatbot(height=400)
    msg_in    = gr.Textbox(label="Your question", placeholder="Type and press Enterâ€¦")
    clear_btn = gr.Button("Clear chat")

    session_state = gr.State([])  # perâ€‘user isolated history

    msg_in.submit(
        fn=chat_fn,
        inputs=[session_state, msg_in],
        outputs=[session_state, chatbot]
    )
    clear_btn.click(lambda: ([], []), outputs=[session_state, chatbot])

demo.queue()        # you can set concurrency_count if desired
demo.launch(share=True, debug=True)
